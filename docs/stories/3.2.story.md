# Story 3.2 – Mock Prompt Call Workflow

## Status

Approved

## Story

**As a** technical evaluator,
**I want** AI buttons to trigger a mocked prompt API call with rich element context,
**so that** I can observe AI prompt generation and error handling without relying on real AI services.

## Acceptance Criteria

From PRD Epic 3 – AI Assist Button & Chatbot Integration:

1. AC1: Button click sends POST to `/mock/ai_generate_ui_prompt` with element context.
2. AC2: Mock server responds with prompt/extraInfo, normalized by the SDK.
3. AC3: Network errors surface in demo UI with retry messaging.

## Tasks / Subtasks

- [ ] Task 1 – Define and reuse shared prompt types (AC: 1, 2)
  - [ ] Ensure `AiPromptRequest` and `AiPromptResponse` types are defined in `packages/shared` and reused by the SDK prompt workflow.  
        [Source: architecture/shared-types-and-api.md#shared-types--api-contract]
  - [ ] Include element identifiers, value, and metadata in the request payload.

- [ ] Task 2 – Implement prompt API client in SDK (AC: 1, 2)
  - [ ] Implement an internal client function (e.g., `requestAiPrompt(config)`) that issues a `POST` to `/mock/ai_generate_ui_prompt` using the shared types.  
        [Source: architecture/shared-types-and-api.md#shared-types--api-contract]
  - [ ] Normalize responses into a consistent `AiPromptResponse` structure and attach a `requestId` for tracing.

- [ ] Task 3 – Wire AI button click to prompt workflow (AC: 1, 2)
  - [ ] On AI button click, gather component context (elementId, label/value, metadata) and construct `AiPromptRequest`.  
        [Source: architecture/ai-overlay.md#ai-assist-button-rendering]
  - [ ] Call the prompt API client and emit a `ChatbotEvent` with the received prompt data.

- [ ] Task 4 – Error handling and retry behavior (AC: 3)
  - [ ] Implement a single retry on network failures with a small delay (e.g., 500ms).  
        [Source: architecture/platform-deployment.md#resilience--monitoring]
  - [ ] Surface errors in the demo via toasts and timeline entries, with a clear “Retry” option for the user.  
        [Source: front-end-spec.md#command-feedback-guidelines]

- [ ] Task 5 – Demo UI indicators for prompt requests (AC: 1, 2, 3)
  - [ ] Add visual loading state on AI buttons and in the chatbot drawer while prompt responses are pending.  
        [Source: front-end-spec.md#animation--micro-interactions]
  - [ ] Ensure all prompt-related activity is visible in the command timeline or a dedicated AI prompt log.

## Dev Notes

### Previous Story Insights

- Builds on AI button factory from Story 3.1 for UI attachment.
- Uses mock services architecture for `/mock/ai_generate_ui_prompt`.  
  [Source: architecture/overview-and-modules.md#module-boundaries]

### Implementation Playbook (Dev Agent)

Follow these steps when implementing this story:

1. **Shared types**
   - Ensure `AiPromptRequest` and `AiPromptResponse` live in shared types and are reused.
   - Include `elementId`, value, metadata, and `requestId` where appropriate.

2. **Prompt API client**
   - Implement a client function (e.g., `requestAiPrompt`) that POSTs to `/mock/ai_generate_ui_prompt` using shared types.
   - Normalize responses into `AiPromptResponse` and attach a `requestId`.

3. **Button click → prompt flow**
   - On AI button click, gather context (elementId, label/value, metadata) and build `AiPromptRequest`.
   - Call the client and emit a `ChatbotEvent` (or equivalent) with the response.

4. **Error handling & retry**
   - Implement a single retry for network failures with a small delay (~500ms).
   - Surface errors via toasts and timeline entries, with a clear retry affordance.

5. **Demo indicators**
   - Add loading states on AI buttons and the chatbot drawer while prompts are in flight.
   - Ensure prompt activity is reflected in logs/timeline.

Dev should keep Tasks/Subtasks and Dev Notes aligned with the implementation; QA Results are for QA.

### Data Models

- Strictly adhere to `AiPromptRequest` and `AiPromptResponse` definitions in shared types.
- Include `requestId` to correlate timeline entries, chatbot messages, and network logs.

### API Specifications

- HTTP POST to `/mock/ai_generate_ui_prompt` as defined in the OpenAPI snippet.  
  [Source: architecture/shared-types-and-api.md#shared-types--api-contract]
- Handle `200` and `400` responses with consistent error object handling.

### Component Specifications

- Chatbot drawer should visually indicate when it’s waiting for a prompt vs. when a prompt has arrived.  
  [Source: front-end-spec.md#user-flows]
- AI buttons should show loading state and disable repeated clicks during in-flight requests.

### File Locations & Structure

- API client: `packages/sdk/src/ai-overlay/prompt-client.ts` (or similar).
- Mock server route: `apps/mocks/api/mock/ai_generate_ui_prompt.ts` (or equivalent).  
  [Source: architecture/overview-and-modules.md#module-boundaries]

### Testing Requirements

- Unit tests for client function handling success and error responses.
- Integration tests verifying end-to-end behavior from button click → mock server → chatbot update.

### Technical Constraints & Considerations

- Respect performance targets; avoid excessive retries or large payloads.  
  [Source: architecture/security-performance-monitoring.md#performance-targets]
- Ensure no secrets or real API keys are involved; mock-only.

## Testing

- Use the demo to trigger AI prompts from various elements and inspect network calls, chatbot results, and error behavior.

## Change Log

| Date       | Version | Description                                   | Author |
| ---------- | ------- | --------------------------------------------- | ------ |
| 2025-11-14 | 0.1.0   | Initial draft of Story 3.2 and task breakdown | SM     |

## Dev Agent Record

_This section is reserved for the Dev Agent to fill in during implementation (agent model, debug log references, completion notes, and file list)._ 

## QA Results

### QA Validation Playbook

When this story is ready for QA review:

1. **Verify Acceptance Criteria directly**
   - AC1: Confirm button clicks send POST requests to `/mock/ai_generate_ui_prompt` with appropriate element context.
   - AC2: Verify mock responses are normalized into prompts/extraInfo and surfaced correctly in the UI.
   - AC3: Simulate network errors and confirm they surface with retry messaging without breaking the app.

2. **Flow visibility**
   - Ensure prompt requests/responses are visible in logs/timeline or a dedicated AI log.

3. **Record QA result**
   - Summarize findings here with status (READY / NEEDS REVISION / BLOCKED) and issues.

_This section is reserved for the QA Agent to record validation results and gate decisions for this story._
